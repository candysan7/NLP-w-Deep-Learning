{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f9c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import glob\n",
    "from pprint import pprint\n",
    "import string\n",
    "from string import digits\n",
    "import statistics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35471766",
   "metadata": {},
   "source": [
    "### Part 1B\n",
    "(b) Data Exploration and Pre-processing\n",
    "i. You can use binary encoding for the sentiments , i.e y = 1 for positive senti-\n",
    "ments and y = -1 for negative sentiments.\n",
    "\n",
    "ii. The data are pretty clean. Remove the punctuation and numbers from the\n",
    "data.\n",
    "\n",
    "iii. The name of each text file starts with cv number. Use text files 0-699 in each\n",
    "class for training and 700-999 for testing.\n",
    "\n",
    "iv. Count the number of unique words in the whole dataset (train + test) and\n",
    "print it out.\n",
    "\n",
    "v. Calculate the average review length and the standard deviation of review\n",
    "lengths. Report the results.\n",
    "\n",
    "vi. Plot the histogram of review lengths.\n",
    "\n",
    "vii. To represent each text (= data point), there are many ways. In NLP/Deep\n",
    "Learning terminology, this task is called tokenization. It is common to rep-\n",
    "resent text using popularity/ rank of words in text. The most common word\n",
    "in the text will be represented as 1, the second most common word will be\n",
    "represented as 2, etc. Tokenize each text document using this method.2\n",
    "\n",
    "viii. Select a review length L that 70% of the reviews have a length below it. If\n",
    "you feel more adventurous, set the threshold to 90%.\n",
    "\n",
    "ix. Truncate reviews longer than L words and zero-pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ec1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in positive: \n",
    "#     counter = 0 \n",
    "#     files = glob.glob(os.path.join('../data/' + i))\n",
    "#     files = ' '.join(map(str, files))\n",
    "#     csv_files = glob.glob(os.path.join(files + \"/*.csv\"))\n",
    "#     csv_files = sorted(csv_files, key=lambda x: int(re.match('\\D*(\\d+)', x).group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb5b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"../Data/pos/\")\n",
    "# for file in glob.glob(\"*.txt\"):\n",
    "#     print(file)\n",
    "    \n",
    "# os.chdir(\"../Data/neg/\")\n",
    "# for file in glob.glob(\"*.txt\"):\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fc6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files_path = glob.glob('../Data/pos/*.txt')\n",
    "neg_files_path = glob.glob('../Data/neg/*.txt')\n",
    "# print(pos_neg_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259860b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364ba5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "#Biii split train and test data \n",
    "pos_files_path_train = pos_files_path[:700]\n",
    "pos_files_path_test = pos_files_path[700:]\n",
    "neg_files_path_train = neg_files_path[:700]\n",
    "neg_files_path_test = neg_files_path[700:]\n",
    "\n",
    "pos_neg_train = pos_files_path_train + neg_files_path_train\n",
    "pos_neg_test = pos_files_path_test + neg_files_path_test\n",
    "print(len(pos_neg_train))\n",
    "print(len(pos_neg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b379c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Unique Words\n",
      "46830\n"
     ]
    }
   ],
   "source": [
    "#Bii remove punctuation and digits \n",
    "#Biv count the total unique words in test + train \n",
    "\n",
    "pos_neg_train_test = pos_neg_train + pos_neg_test\n",
    "length_text_train_test = []\n",
    "unique_words_train_test = [] \n",
    "\n",
    "for i in pos_neg_train_test: \n",
    "    text = open(i, \"r\").read()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # this removes punctuation \n",
    "    text = text.translate(str.maketrans('', '', string.digits)) # this removs number \n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    text_length = text.split(' ') ### this splits to calculate the number of words \n",
    "#     text_length = (len(text_length)) ### this finds number of words \n",
    "    length_text_train_test.append(len(text_length))\n",
    "#     pprint(text)\n",
    "\n",
    "##### this is to find the number of unique words\n",
    "    for word in text.split(' '):\n",
    "        word = word.strip()\n",
    "        if word not in unique_words_train_test: \n",
    "            unique_words_train_test.append(word)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "#     break\n",
    "print('Total Data Unique Words')\n",
    "print(len(unique_words_train_test))\n",
    "# print(length_text_train_test)\n",
    "#####-----------------------------------TEST DATA--------------------------------------------------------\n",
    "# length_text_test = []\n",
    "# unique_words_test = [] \n",
    "\n",
    "# for i in pos_neg_test: \n",
    "#     text = open(i, \"r\").read()\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     text = text.translate(str.maketrans('', '', string.digits))\n",
    "#     text = text.strip()\n",
    "#     text = \" \".join(text.split())\n",
    "#     text_length = text.split(' ') ### this splits to calculate the number of words \n",
    "# #     text_length = (len(text_length)) ### this finds number of words \n",
    "#     length_text_test.append(len(text_length))\n",
    "# #     pprint(text)\n",
    "    \n",
    "# #     text = list(text)\n",
    "# #     print(len(text))\n",
    "\n",
    "# ##### this is to find the number of unique words\n",
    "#     for word in text.split(' '):\n",
    "#         word = word.strip()\n",
    "#         if word not in unique_words_test: \n",
    "#             unique_words_test.append(word)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "# #     break\n",
    "# print('Test Data Unique Words')\n",
    "# print(len(unique_words_test))\n",
    "# # print(length_text_test)\n",
    "# print('Total Data - Unique Words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ae89e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data\n",
      "Average Length: 644.3555\n",
      "Standard Deviation: 285.0511431249635\n"
     ]
    }
   ],
   "source": [
    "### B V. Calculate average review length and the standard adeviation of review lenghts \n",
    "\n",
    "standard_dev = statistics.stdev(length_text_train_test)\n",
    "average_length = sum(length_text_train_test) / len(length_text_train_test)\n",
    "print('All Data')\n",
    "print(\"Average Length:\", average_length)\n",
    "print(\"Standard Deviation:\",standard_dev)\n",
    "\n",
    "# print('\\n')\n",
    "# standard_dev = statistics.stdev(length_text_test)\n",
    "# average_length = sum(length_text_test) / len(length_text_test)\n",
    "# print('Test Data')\n",
    "# print(\"Average Length:\", average_length)\n",
    "# print(\"Standard Deviation:\",standard_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49397f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### vi Plot the histogram of review lengths of all data(train+test).\n",
    "\n",
    "# lenth_text_train_test = length_text_train + length_text_test #ploting length of all data \n",
    "\n",
    "plt.hist(length_text_train_test)\n",
    "plt.title('All Data - Histogram of Review Lengths')\n",
    "plt.xlabel(\"Number of Words in Text File\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e110a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### incase we want to use stop words \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sentence = \"Hi Test This is a sentence hello and we are\"\n",
    "def remove_stopwords(text): \n",
    "    text_tokens = word_tokenize(text)\n",
    "    text_no_stopwords = [i for i in text_tokens if not i in stopwords.words(\"english\")]\n",
    "    no_stopwords_str = (\" \").join(text_no_stopwords)\n",
    "\n",
    "    return no_stopwords_str\n",
    "\n",
    "# print(remove_stopwords(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499d67b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Percentile Review Length: 737.0\n",
      "996\n",
      "996\n",
      "996\n",
      "408\n",
      "408\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "### bi encode y=1 positive y =0 negative \n",
    "# B viii select review length at 70% \n",
    "# B Vii tokenize the train data and test data (see below for better tokenization (tokenize entire xtrain tog))  \n",
    "# B ix truncate reviews and padding shorter than L \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "##### REMOVING STOP WORDS ATTEMPT \n",
    "text_length_70 = np.percentile(length_text_train_test, 70)  ### finding L \n",
    "print(\"70 Percentile Review Length:\",text_length_70) # this is review length at 70 % for all dat \n",
    "\n",
    "x_train_70L_tokens = []\n",
    "x_train_70L = []\n",
    "y_train_70L = []\n",
    "pos_neg_train_70L = [] # file paths for text that falls within the 70% percentile \n",
    "for i in pos_neg_train: \n",
    "    text = open(i, \"r\").read()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    text_length = str(text)\n",
    "    text_length = text_length.split(' ') ### this splits to calculate the number of words \n",
    "    text_length = (len(text_length)) ### this finds number of words aka length\n",
    "    \n",
    "#     text = remove_stopwords(text) ##### this removes the stop words \n",
    "    text = [text]\n",
    "#     print(text)\n",
    "#     break\n",
    "    if text_length <= text_length_70: \n",
    "        pos_neg_train_70L.append(i)\n",
    "        x_train_70L.append(text)\n",
    "        \n",
    "        ### tokenize #### this was not used for the rest of the model\n",
    "        t = Tokenizer() ### this tokenizes for each text, (was not used for this hw) # B Vii tokenize the train data and test data  \n",
    "        t.fit_on_texts(text)\n",
    "        token_text = t.texts_to_sequences(text)\n",
    "        token_text = pad_sequences(token_text, maxlen=737,padding='post') # pads 0 to the right # B ix padding shorter than L \n",
    "        x_train_70L_tokens.append(token_text)\n",
    "        \n",
    "        if 'pos' in i:  # encoding 1 for pos \n",
    "            y_train_70L.append(1)\n",
    "        elif 'neg' in i: # encoding 0 for neg\n",
    "            y_train_70L.append(0)\n",
    "            \n",
    "# print(len(x_train_70L_tokens)) ### THIS IS THE TEXT TOKENIZED            \n",
    "print(len(x_train_70L)) ### THIS IS THE TEXT (NOT TOKENIZED)        \n",
    "print(len(pos_neg_train_70L)) #### THIS IS THE FILE PATH \n",
    "print(len(y_train_70L))\n",
    "##### ----- FINDING TEST PATHS WITHIN THE 70 PERCENTILE ------\n",
    "\n",
    "x_test_70L_tokens = []\n",
    "x_test_70L = []\n",
    "y_test_70L = [] \n",
    "pos_neg_test_70L=[] # file paths for text that falls within the 70% percentile \n",
    "for i in pos_neg_test: \n",
    "    text = open(i, \"r\").read()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    text_length = str(text)\n",
    "    text_length = text_length.split(' ') ### this splits to calculate the number of words \n",
    "    text_length = (len(text_length)) ### this finds number of words aka length\n",
    "#     text = remove_stopwords(text) #### this removes stop words\n",
    "    text = [text]\n",
    "    if text_length <= text_length_70: \n",
    "        pos_neg_test_70L.append(i)\n",
    "        x_test_70L.append(text)\n",
    "        \n",
    "        ### tokenize \n",
    "        t = Tokenizer()\n",
    "        t.fit_on_texts(text)\n",
    "        token_text = t.texts_to_sequences(text)\n",
    "        token_text = pad_sequences(token_text, maxlen=737,padding='post')\n",
    "        x_test_70L_tokens.append(token_text)\n",
    "        \n",
    "        if 'pos' in i:\n",
    "            y_test_70L.append(1)\n",
    "        elif 'neg' in i: \n",
    "            y_test_70L.append(0)\n",
    "                        \n",
    "# print(len(x_test_70L_tokens))            \n",
    "print(len(x_test_70L))  # length is not exactly 70% as we found the cut off length based on all data then cut off based on train and test data     \n",
    "print(len(pos_neg_test_70L))\n",
    "print(len(y_test_70L))\n",
    "\n",
    "x_train_70L_tokens = np.vstack(x_train_70L_tokens) ### change the name later \n",
    "x_test_70L_tokens = np.vstack(x_test_70L_tokens)\n",
    "y_train_70L = np.array(y_train_70L)\n",
    "y_test_70L = np.array(y_test_70L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a4d2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n",
      "[[17037  3797    30 ...     0     0     0]\n",
      " [ 8424   131     3 ...     0     0     0]\n",
      " [10012   280  4410 ...     0     0     0]\n",
      " ...\n",
      " [31284   598   548 ...     0     0     0]\n",
      " [ 6642  1631     2 ...     0     0     0]\n",
      " [14392    19    50 ...     0     0     0]]\n",
      "408\n",
      "[[  762    71     7 ...     0     0     0]\n",
      " [ 7078   287  1918 ...     0     0     0]\n",
      " [10113  5416  1764 ...     0     0     0]\n",
      " ...\n",
      " [  762   314     7 ...     0     0     0]\n",
      " [ 3499    32     2 ...     0     0     0]\n",
      " [19951   562   482 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "#### tokenizing x train then x test\n",
    "# we used this tokenzied data set for the models \n",
    "\n",
    "# B Vii tokenize the train data and test data  together \n",
    "# B ix padding shorter than L \n",
    "df_train = pd.DataFrame(columns=['xtrain_Text'])\n",
    "df_train['xtrain_Text'] = x_train_70L\n",
    "df_test = pd.DataFrame(columns=['xtest_Text'])\n",
    "df_test['xtest_Text'] = x_test_70L\n",
    "\n",
    "# df_test['xtest_Text'] = [str(i) for i in df_test['xtest_Text']]\n",
    "\n",
    "x_train_70L = [str(i) for i in x_train_70L]\n",
    "# x_test_70L = [str(i) for i in x_train_70L]\n",
    "# x_train_test_70L = x_train_70L + x_test_70L\n",
    "# x_train_test_70L = [str(i) for i in x_train_70L]\n",
    "\n",
    "# test = ['hello andy xiang','this andy is me','where did that ']\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x_train_70L)\n",
    "\n",
    "token_total_xtrain = t.texts_to_sequences(x_train_70L)\n",
    "token_total_xtrain = pad_sequences(token_total_xtrain, maxlen=737, padding='post') # pads 0 to the right \n",
    "print(len(token_total_xtrain))\n",
    "\n",
    "print(token_total_xtrain)\n",
    "\n",
    "x_test_70L = [str(i) for i in x_test_70L]\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x_test_70L)\n",
    "\n",
    "token_total_xtest = t.texts_to_sequences(x_test_70L)\n",
    "token_total_xtest = pad_sequences(token_total_xtest, maxlen=737, padding='post') # pads 0 to the right \n",
    "print(len(token_total_xtest))\n",
    "\n",
    "print(token_total_xtest)\n",
    "\n",
    "### 996 \n",
    "### 408 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4fb7d",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8af399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 737, 32)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 23584)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 23585     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,585\n",
      "Trainable params: 183,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "32/32 [==============================] - 1s 10ms/step - loss: 0.6942 - acc: 0.5291\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6051 - acc: 0.8876\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.5336 - acc: 0.8122\n",
      "Train Acc 0.8122490048408508\n",
      "Epoch 1/2\n",
      "13/13 [==============================] - 1s 9ms/step - loss: 0.6987 - acc: 0.5441\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6098 - acc: 0.9485\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5562 - acc: 0.8750\n",
      "Test Acc 0.875\n"
     ]
    }
   ],
   "source": [
    "### word embedding problem \n",
    "#### https://www.analyticsvidhya.com/blog/2021/06/part-7-step-by-step-guide-to-master-nlp-word-embedding/\n",
    "#### https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "#### https://www.tensorflow.org/text/guide/word_embeddings \n",
    "#### https://towardsdatascience.com/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633\n",
    "#### https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/?fbclid=IwAR2L7-2TZUDISgYgoLmfgVM7vYlprD7BL7J13T0U4QM3bmqHyRFaEi_WaIE\n",
    "\n",
    "\n",
    "x_train_70L = [str(i) for i in x_train_70L] ### text for all xtrain \n",
    "vocab_size = 5000\n",
    "embeded_text_xtrain = [one_hot(a, vocab_size) for a in x_train_70L]\n",
    "# print((embeded_text_xtrain)[0])\n",
    "\n",
    "padded_text_xtrain = pad_sequences(embeded_text_xtrain, maxlen=737, padding='post')\n",
    "# embed_xtrain.append(padded_text)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length = padded_text_xtrain.shape[1])) #### 32 * 737 MATRIX \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "model.fit(padded_text_xtrain, y_train_70L, epochs=2)\n",
    "\n",
    "train_acc = model.evaluate(padded_text_xtrain, y_train_70L)\n",
    "print('Train Acc',train_acc[1])\n",
    "\n",
    "\n",
    "##### this is for test data --------------------------\n",
    "x_test_70L = [str(i) for i in x_test_70L] ### text for all xtrain \n",
    "vocab_size = 5000\n",
    "embeded_text_xtest = [one_hot(a, vocab_size) for a in x_test_70L]\n",
    "# print((embeded_text_xtest)[0])\n",
    "\n",
    "padded_text_xtest = pad_sequences(embeded_text_xtest, maxlen=737, padding='post')\n",
    "# embed_xtrain.append(padded_text)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=32, input_length = padded_text_xtrain.shape[1])) #### 32 * 737 MATRIX \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# print(model.summary())\n",
    "model.fit(padded_text_xtest, y_test_70L, epochs=2)\n",
    "test_acc = model.evaluate(padded_text_xtest, y_test_70L)\n",
    "print('Test Acc',test_acc[1])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# print(model.summary())\n",
    "# model.fit(padded_text_xtrain, y_train_70L, epochs=2)\n",
    "\n",
    "# train_acc = model.evaluate(padded_text_xtrain, y_train_70L)\n",
    "# print('Train Acc',train_acc[1])\n",
    "\n",
    "# print(model.summary()) ### for test data \n",
    "# print((embeded_text_xtrain)[0])\n",
    "# break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0ec63",
   "metadata": {},
   "source": [
    "### D. Mulit-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8829ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_total_xtrain[i] = np.where(token_total_xtrain[i] > 5000, 0, token_total_xtrain[i])\n",
    "######## THIS REMOVES TOKENS  THAT ARE GREATER THAN 5000 \n",
    "\n",
    "x_train5000=[np.array(i) for i in token_total_xtrain]\n",
    "x_train5000 = [(np.where(i > 5000, 0, i)) for i in x_train5000]\n",
    "\n",
    "x_test5000=[np.array(i) for i in token_total_xtest]\n",
    "x_test5000 = [(np.where(i > 5000, 0, i)) for i in x_test5000]\n",
    "\n",
    "x_train5000= np.array(x_train5000)\n",
    "x_test5000= np.array(x_test5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34392bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 2s 11ms/step - loss: 0.6955 - accuracy: 0.5261\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.6824 - accuracy: 0.5703\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 23584)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                1179250   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,344,433\n",
      "Trainable params: 1,344,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Train + Test Results\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.5963 - accuracy: 0.8484\n",
      "Train Accuracy: 0.8483935594558716\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5098\n",
      "Test Accuracy: 0.5098039507865906\n"
     ]
    }
   ],
   "source": [
    "#### Multi-Layer Perceptron problem \n",
    "###https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af\n",
    "#### https://becominghuman.ai/1-dimensional-convolution-layer-for-nlp-task-5d2e86e0229c\n",
    "#### https://keras.io/api/models/model_training_apis/ \n",
    "#### https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5001, output_dim=32, input_length = padded_text_xtrain.shape[1]))\n",
    "model.add(Flatten())\n",
    "# model.add(Input(shape=(737, ))) ### 737 is the shpe \n",
    "model.add(Dense(50, activation='relu')) ### 50 ReLus Layer 1\n",
    "model.add(Dropout(0.20)) #20% on the first layer \n",
    "model.add(Dense(50, activation='relu')) # layer 2 \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu')) # layer 3 (3 dense hidden layers)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  #this is outer layer with sigmoid #### this is the output layer\n",
    "# model.add(Dropout(0.5)) ### removed this drop out to increase accuracy\n",
    "\n",
    "# model.compile(optimizer='adam', loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "              \n",
    "model.fit(x_train5000, y_train_70L, batch_size=10, epochs=2)\n",
    "#          validation_data = (x_test_70L_tokens,y_test_70L))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('\\nTrain + Test Results')\n",
    "train_acc = model.evaluate(x_train5000,y_train_70L)\n",
    "print('Train Accuracy:',train_acc[1])\n",
    "test_acct = model.evaluate(x_test5000,y_test_70L)\n",
    "print('Test Accuracy:',test_acct[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fb1fc",
   "metadata": {},
   "source": [
    "### E. CNN Problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c30e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# token_xtrain, y_train_70L = shuffle(token_xtrain, y_train_70L, random_state=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a067266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 3s 14ms/step - loss: 0.7047 - accuracy: 0.5221\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.6790 - accuracy: 0.5873\n",
      "\n",
      "Train + Test Results\n",
      "32/32 [==============================] - 1s 10ms/step - loss: 0.6244 - accuracy: 0.6998\n",
      "Train Accuracy: 0.6997991800308228\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 0.7001 - accuracy: 0.4926\n",
      "Test Accuracy: 0.49264705181121826\n"
     ]
    }
   ],
   "source": [
    "###  CNN problem \n",
    "##https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "####https://www.analyticsvidhya.com/blog/2021/06/natural-language-processing-sentiment-analysis-using-lstm/\n",
    "####https://becominghuman.ai/1-dimensional-convolution-layer-for-nlp-task-5d2e86e0229c\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5001, output_dim=32, input_length = 737)) ### change to 50000 input dimension to make flatten work\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "# model.add(Dropout(0.20)) #lower performance \n",
    "\n",
    "model.add(Flatten())#### this usually works better\n",
    "model.add(Dense(50, activation='relu')) #### kernel_constraint=max_norm(3))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5)) #### remove this drop out to increase acc\n",
    "\n",
    "# model.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=\"binary_crossentropy\",metrics=['accuracy']) ## TRY 0.01\n",
    "##### adjusted learning rate for better acc (0.01 is best acc)\n",
    "\n",
    "# print(model.summary())\n",
    "model.fit(x_train5000, y_train_70L, epochs=2, batch_size=10,shuffle=True)\n",
    "\n",
    "print('\\nTrain + Test Results')\n",
    "train_acc = model.evaluate(x_train5000,y_train_70L)\n",
    "print('Train Accuracy:',train_acc[1])\n",
    "test_acct = model.evaluate(x_test5000,y_test_70L)\n",
    "print('Test Accuracy:',test_acct[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe7037",
   "metadata": {},
   "source": [
    "### F. LSTM Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "367f6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 737, 32)           160032    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177,057\n",
      "Trainable params: 177,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 38s 360ms/step - loss: 0.6948 - accuracy: 0.5181\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.6936 - accuracy: 0.5291\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 35s 352ms/step - loss: 0.6923 - accuracy: 0.5191\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 35s 350ms/step - loss: 0.6919 - accuracy: 0.5261\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 29s 285ms/step - loss: 0.6949 - accuracy: 0.5231\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 0.6918 - accuracy: 0.5361\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 0.6920 - accuracy: 0.5462\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 0.6947 - accuracy: 0.5392\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 28s 283ms/step - loss: 0.6969 - accuracy: 0.5412\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 0.6927 - accuracy: 0.5301\n",
      "\n",
      "Train + Test Results\n",
      "32/32 [==============================] - 3s 72ms/step - loss: 0.6903 - accuracy: 0.5371\n",
      "Train Accuracy: 0.5371485948562622\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 0.6894 - accuracy: 0.5441\n",
      "Test Accuracy: 0.5441176295280457\n"
     ]
    }
   ],
   "source": [
    "### ASKED TA QINGYANG ZHANG - we could not figure out why accuracy is not significalt increasing, \n",
    "####### said he would grade based on structure of the model rather than accuracy \n",
    "### LSTM\n",
    "####https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "####https://www.analyticsvidhya.com/blog/2021/06/natural-language-processing-sentiment-analysis-using-lstm/\n",
    "###https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(Embedding(5001, 32, input_length = x_train5000.shape[1])) ### first layer uses 32 length vector \n",
    "# model.add(LSTM(32,dropout=0.2)) ### same way to declare dropout \n",
    "LSTM_model.add(LSTM(32)) ### removed activation layer \n",
    "LSTM_model.add(Dropout(0.2))\n",
    "LSTM_model.add(Flatten()) ### maybe not needed \n",
    "\n",
    "LSTM_model.add(Dense(256,activation='relu')) #256 ReLus\n",
    "LSTM_model.add(Dropout(0.2))\n",
    "LSTM_model.add(Dense(1,activation='sigmoid'))  ### added sigmoid layer based on the diagram \n",
    "\n",
    "LSTM_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "#0.0000001  ### try learning ratee 0.1\n",
    "# model.compile(optimizer=custom_opt,loss='binary_crossentropy',  metrics = ['accuracy'])\n",
    "print(LSTM_model.summary())\n",
    "\n",
    "LSTM_model.fit(x_train5000, y_train_70L, epochs=10, batch_size=10,shuffle=True) ### using 10 epochs as it takes the shortest time \n",
    "\n",
    "print(\"\\nTrain + Test Results\")\n",
    "train_acc = LSTM_model.evaluate(x_train5000,y_train_70L)\n",
    "print('Train Accuracy:',train_acc[1])\n",
    "test_acct = LSTM_model.evaluate(x_test5000,y_test_70L)\n",
    "print('Test Accuracy:',test_acct[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "1144.83px",
    "left": "2101px",
    "right": "20px",
    "top": "142px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
